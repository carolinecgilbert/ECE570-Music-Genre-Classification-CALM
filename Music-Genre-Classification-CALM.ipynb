{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":10054,"status":"ok","timestamp":1731268805003,"user":{"displayName":"Caroline Gilbert","userId":"01576484869533418489"},"user_tz":300},"id":"fFG7p8rMsk-p"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","A module that was compiled using NumPy 1.x cannot be run in\n","NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n","versions of NumPy, modules must be compiled with NumPy 2.0.\n","Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n","\n","If you are a user of the module, the easiest solution will be to\n","downgrade to 'numpy<2' or try to upgrade the affected module.\n","We expect that some modules will need time to support NumPy 2.\n","\n","Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n","  File \"<frozen runpy>\", line 88, in _run_code\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n","    app.launch_new_instance()\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n","    app.start()\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n","    self.io_loop.start()\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n","    self.asyncio_loop.run_forever()\n","  File \"/Users/carolinegilbert/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n","    self._run_once()\n","  File \"/Users/carolinegilbert/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n","    handle._run()\n","  File \"/Users/carolinegilbert/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n","    self._context.run(self._callback, *self._args)\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n","    await self.process_one()\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n","    await dispatch(*args)\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n","    await result\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n","    await super().execute_request(stream, ident, parent)\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n","    reply_content = await reply_content\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n","    res = shell.run_cell(\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n","    return super().run_cell(*args, **kwargs)\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n","    result = self._run_cell(\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n","    result = runner(coro)\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n","    if await self.run_code(code, result, async_=asy):\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"/var/folders/3j/2hpw7ptj4r565s37bww2_dhw0000gn/T/ipykernel_30621/2948228525.py\", line 3, in <module>\n","    import torch\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n","    from .functional import *  # noqa: F403\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n","    import torch.nn.functional as F\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n","    from .modules import *  # noqa: F403\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n","    from .transformer import TransformerEncoder, TransformerDecoder, \\\n","  File \"/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n","    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n","/Users/carolinegilbert/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n","  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n","Matplotlib is building the font cache; this may take a moment.\n"]}],"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","from sklearn.ensemble import RandomForestClassifier\n","import librosa\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, random_split, ConcatDataset, Subset\n","from sklearn.model_selection import KFold\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":197,"status":"ok","timestamp":1731268838939,"user":{"displayName":"Caroline Gilbert","userId":"01576484869533418489"},"user_tz":300},"id":"VIDGuKZIuWxu"},"outputs":[],"source":["# Configuration\n","config = {\n","    'batch_size': 32,\n","    'learning_rate': 0.1,\n","    'num_epochs': 150,\n","    'hidden_dim': 256,\n","    'latent_dim': 16,\n","    'input_dim': 64,\n","    'num_classes': 10,\n","    'weight_decay': 1e-4,\n","    'sample_rate': 22050,\n","    'n_mels': 128,\n","    'n_fft': 2048,\n","    'hop_length': 1024,\n","    'duration': 30,\n","}"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":11475,"status":"ok","timestamp":1731269008864,"user":{"displayName":"Caroline Gilbert","userId":"01576484869533418489"},"user_tz":300},"id":"64Vos1ow7we_"},"outputs":[],"source":["# Dataset class for DataLoader\n","class GTZANDataset(Dataset):\n","    def __init__(self, data, labels, transform=None):\n","        self.data = data\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        sample = torch.tensor(self.data[idx], dtype=torch.float32).unsqueeze(0)\n","        label = torch.tensor(self.labels[idx], dtype=torch.long)\n","        if self.transform:\n","            sample = self.transform(sample)\n","        return sample, label\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EtKNMvRvkcqH"},"source":["#CALM Classification Model\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":647,"status":"ok","timestamp":1731269064460,"user":{"displayName":"Caroline Gilbert","userId":"01576484869533418489"},"user_tz":300},"id":"-v2FkBgEwfSS"},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        # Define the 4 convolutional layers with pooling sizes from paper\n","        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n","        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n","\n","        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","        self.pool2 = nn.MaxPool2d(kernel_size=(3, 3))\n","\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","        self.pool3 = nn.MaxPool2d(kernel_size=(4, 4))\n","\n","        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","        self.pool4 = nn.MaxPool2d(kernel_size=(4, 2))\n","        # Reduces frequency by 4 and time by 2 instead of 4\n","        # to reach 13 instead of 15 (possible error in paper)\n","\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        #print(\"CNN Input shape: \", x.shape)\n","        # Apply convolution, activation, pooling, and dropout in each layer\n","        x = self.dropout(self.pool1(F.relu(self.conv1(x))))\n","        x = self.dropout(self.pool2(F.relu(self.conv2(x))))\n","        x = self.dropout(self.pool3(F.relu(self.conv3(x))))\n","        x = self.dropout(self.pool4(F.relu(self.conv4(x))))\n","\n","        return x\n","\n","\n","class LSTMAutoencoder(nn.Module):\n","    def __init__(self):\n","        super(LSTMAutoencoder, self).__init__()\n","        # Encoder LSTM with units {256, 64, 16}\n","        self.encoder_lstm1 = nn.LSTM(64, 256, batch_first=True, dropout=0.5)\n","        self.encoder_lstm2 = nn.LSTM(256, 64, batch_first=True, dropout=0.5)\n","        self.encoder_lstm3 = nn.LSTM(64, 16, batch_first=True, dropout=0.5)\n","\n","        # Decoder LSTM with units {16, 64, 256}\n","        self.decoder_lstm1 = nn.LSTM(16, 64, batch_first=True, dropout=0.5)\n","        self.decoder_lstm2 = nn.LSTM(64, 256, batch_first=True, dropout=0.5)\n","        self.decoder_lstm3 = nn.LSTM(256, 64, batch_first=True, dropout=0.5)\n","\n","    def forward(self, x):\n","        #print(\"LSTM shape before encoding: \", x.shape)\n","        batch_size, time_steps, features = x.size()  # Input is [32, 1, 13]\n","        # Encode\n","        output, (h_n1, c_n1) = self.encoder_lstm1(x)\n","        output, (h_n2, c_n2) = self.encoder_lstm2(output)\n","        output, (h_n3, c_n3) = self.encoder_lstm3(output)\n","\n","\n","        # Decoder initialization with encoded state\n","        decoder_input = h_n3.expand(time_steps, batch_size, -1)  # Repeat across time steps for decoding\n","        decoder_output, _ = self.decoder_lstm1(decoder_input)\n","        decoder_output, _ = self.decoder_lstm2(decoder_output)\n","        decoder_output, _ = self.decoder_lstm3(decoder_output)\n","\n","        # Interpolate to match original input dimensions\n","        decoder_output = decoder_output.permute(1, 0, 2) \n","        decoder_output = decoder_output.view(batch_size, 1, 13, 64)  \n","        decoder_output = F.interpolate(decoder_output, size=(128, 647), mode='bilinear', align_corners=False)\n","\n","        # Return reconstruction and encoded state\n","        return decoder_output, h_n3\n","\n","class CNN_LSTM(nn.Module):\n","    def __init__(self):\n","        super(CNN_LSTM, self).__init__()\n","        self.cnn = CNN()\n","        self.lstm_autoencoder = LSTMAutoencoder()\n","\n","    def forward(self, x):\n","        cnn_output = self.cnn(x)\n","        lstm_input = cnn_output.view(x.size(0), 13, 64)  \n","        reconstruction, encoded_state = self.lstm_autoencoder(lstm_input)\n","\n","        return reconstruction, encoded_state\n","\n","# Input augmentation functions\n","def initialize_random_clusters(data, n_clusters=10):\n","    n_samples = data.size(0)\n","    initial_indices = torch.randperm(n_samples)[:n_clusters]\n","    cluster_centers = data[initial_indices]\n","    return cluster_centers\n","\n","def assign_clusters(data, cluster_centers):\n","    distances = torch.cdist(data, cluster_centers)\n","    cluster_assignments = distances.argmin(dim=1)\n","    return cluster_assignments\n","\n","def augment_data_with_clusters(batch_data, n_clusters=10):\n","    batch_size = batch_data.size(0)\n","    flattened_data = batch_data.view(batch_size, -1)\n","    cluster_centers = initialize_random_clusters(flattened_data, n_clusters=n_clusters)\n","    cluster_assignments = assign_clusters(flattened_data, cluster_centers)\n","    one_hot_clusters = F.one_hot(cluster_assignments, num_classes=n_clusters).float()\n","    augmented_batch = torch.cat([flattened_data, one_hot_clusters], dim=1)\n","    return augmented_batch, cluster_assignments, cluster_centers\n","\n","# FNN model\n","class FeedForwardNN(nn.Module):\n","    def __init__(self, input_size, feature_size):\n","        super(FeedForwardNN, self).__init__()\n","        self.fc1 = nn.Linear(input_size, 128)\n","        self.fc2 = nn.Linear(128, 32)\n","        self.fc3 = nn.Linear(32, 10)\n","        self.ln1 = nn.LayerNorm(128)\n","        self.ln2 = nn.LayerNorm(32)\n","        self.leaky_relu = nn.LeakyReLU()\n","        self.feature_size = feature_size\n","\n","    def forward(self, x):\n","        #print(\"FNN input shape: \", x.shape)\n","        x = self.leaky_relu(self.ln1(self.fc1(x)))\n","        x = self.leaky_relu(self.ln2(self.fc2(x)))\n","        x = self.fc3(x)\n","        return F.softmax(x, dim=1)\n","\n","    def calculate_dissimilarity(self, input_data, cluster_centers):\n","        # Use only the first 16 dimensions for input_data and cluster_centers\n","        input_data = input_data[:, :16]\n","        cluster_centers = cluster_centers[:, :16]\n","\n","        n_samples, n_features = input_data.shape  \n","        n_clusters = cluster_centers.size(0)\n","\n","        weight_matrix = self.fc1.weight[:, :16].abs().mean(dim=0)\n","\n","        dissimilarity_matrix = torch.zeros(n_samples, n_clusters, device=input_data.device)\n","        for i in range(n_samples):\n","            for l in range(n_clusters):\n","                distance = (weight_matrix * torch.abs(input_data[i] - cluster_centers[l])).sum()\n","                dissimilarity_matrix[i, l] = distance\n","\n","        return dissimilarity_matrix\n","\n","\n","# CALM Classifier integrating FNN and RandomForestClassifier\n","class CALMClassifier(nn.Module):\n","    def __init__(self, fnn_input_size, feature_size, n_clusters=10):\n","        super(CALMClassifier, self).__init__()\n","        self.fnn = FeedForwardNN(fnn_input_size, feature_size)\n","        self.random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n","        self.n_clusters = n_clusters\n","\n","    def forward(self, x):\n","        return self.fnn(x)\n","\n","    def update_random_forest(self, data, cluster_labels):\n","        flattened_data = data.view(data.size(0), -1).cpu().numpy()\n","        self.random_forest.fit(flattened_data, cluster_labels.cpu().numpy())\n","\n","    def predict_cluster_probabilities(self, data):\n","        flattened_data = data.view(data.size(0), -1).cpu().numpy()\n","        cluster_probs = self.random_forest.predict_proba(flattened_data)\n","        return torch.tensor(cluster_probs).to(data.device)\n","\n","    def calculate_dissimilarity(self, input_data, cluster_centers):\n","        return self.fnn.calculate_dissimilarity(input_data, cluster_centers)\n","\n","\n","\n","class FullCALMModel(nn.Module):\n","    def __init__(self, cnn_lstm, device, n_clusters=10):\n","        super(FullCALMModel, self).__init__()\n","        self.cnn_lstm = cnn_lstm\n","        self.calm_classifier = CALMClassifier(fnn_input_size=16 + n_clusters, feature_size=16 + n_clusters)\n","        self.n_clusters = n_clusters\n","        self.device = device\n","\n","    def forward(self, x, cluster_labels):\n","        _, encoded_state = self.cnn_lstm(x)\n","        encoded_state = encoded_state.squeeze(0)\n","        standardized_data = (encoded_state - encoded_state.mean(dim=1, keepdim=True)) / \\\n","                            (encoded_state.std(dim=1, keepdim=True) + 1e-8)\n","        one_hot_clusters = F.one_hot(cluster_labels, num_classes=self.n_clusters).float()\n","        augmented_data = torch.cat([standardized_data, one_hot_clusters.to(self.device)], dim=1)\n","        calm_output = self.calm_classifier(augmented_data)\n","        return calm_output, augmented_data\n","\n","# Train CNN_LSTM\n","def trainCNN_LSTM(model, dataloader, device, num_epochs=20, learning_rate=0.001, weight_decay=1e-4):\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","    criterion = nn.MSELoss()\n","\n","    model.train()\n","    losses = []\n","    total_samples = 0\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        for data, _ in dataloader:\n","            data = data.to(device)\n","\n","            # Forward pass\n","            reconstruction, _ = model(data)\n","\n","            # Calculate loss (RMSE between input and reconstruction)\n","            loss = torch.sqrt(criterion(reconstruction, data))\n","            epoch_loss += loss.item()\n","\n","            # Backward pass and optimize\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            total_samples += data.size(0)\n","\n","        losses.append(epoch_loss/total_samples)\n","\n","        # Print loss per epoch\n","        print(f'(CNN_LSTM) Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(dataloader)}')\n","    print(\"CNN_LSTM Training complete.\")\n","\n","def train_full_model(model, train_loader, optimizer, criterion, num_epochs, max_no_change_epochs=10):\n","    device = model.device\n","    model.train()\n","\n","    # Step 1: Initialize cluster centers on CPU to save GPU memory\n","    all_encoded_states = []\n","\n","    # Get all encoded states from the CNN_LSTM output\n","    with torch.no_grad():\n","        for batch_data, _ in train_loader:\n","            batch_data = batch_data.to(device)\n","            _, encoded_state = model.cnn_lstm(batch_data)\n","            all_encoded_states.append(encoded_state.squeeze(0).cpu()) \n","\n","    # Concatenate all encoded states for clustering\n","    all_encoded_states = torch.cat(all_encoded_states, dim=0)\n","\n","    # Initialize clusters using the encoded states\n","    cluster_centers = initialize_random_clusters(all_encoded_states, n_clusters=model.n_clusters)\n","    cluster_assignments = assign_clusters(all_encoded_states, cluster_centers)\n","\n","    no_change_count = 0\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0.0\n","\n","        # Step 2: Training the FNN and clustering model\n","        for cluster_id in range(model.n_clusters):\n","            optimizer.zero_grad()\n","\n","            # Select the data points belonging to the current cluster\n","            cluster_mask = (cluster_assignments == cluster_id)\n","            cluster_data = all_encoded_states[cluster_mask].to(device)\n","            cluster_labels = torch.full((cluster_data.size(0),), cluster_id, dtype=torch.long, device=device)\n","\n","            # Cluster_data has shape [batch_size, feature_dim] for FullCALMModel processing\n","            if cluster_data.size(0) > 0:\n","                # Augment the data with one-hot encoded cluster labels\n","                one_hot_clusters = F.one_hot(cluster_labels, num_classes=model.n_clusters).float()\n","                augmented_data = torch.cat([cluster_data, one_hot_clusters.to(device)], dim=1)\n","\n","                # Forward pass\n","                outputs = model.calm_classifier(augmented_data)\n","                loss = criterion(outputs, cluster_labels)\n","                loss.backward()\n","                optimizer.step()\n","                epoch_loss += loss.item() * cluster_data.size(0)\n","            else:\n","                print(f\"Cluster {cluster_id} has no samples.\")\n","\n","            torch.cuda.empty_cache() \n","\n","        # Step 3: Update cluster centers and reassign clusters\n","        new_cluster_centers = []\n","        for cluster_id in range(model.n_clusters):\n","            cluster_data = all_encoded_states[cluster_assignments == cluster_id]\n","            if cluster_data.size(0) > 0:\n","                new_center = cluster_data.mean(dim=0)\n","            else:\n","                new_center = cluster_centers[cluster_id] \n","            new_cluster_centers.append(new_center)\n","        new_cluster_centers = torch.stack(new_cluster_centers)\n","\n","        # Calculate dissimilarity matrix and update cluster assignments\n","        dissimilarity_matrix = model.calm_classifier.calculate_dissimilarity(all_encoded_states[:, :16].to(device), new_cluster_centers[:, :16].to(device))\n","        new_cluster_assignments = dissimilarity_matrix.argmin(dim=1).cpu()\n","\n","        # Check for consecutive unchanged cluster assignments\n","        if torch.equal(new_cluster_assignments, cluster_assignments):\n","            no_change_count += 1\n","            print(f\"No change in cluster assignments for {no_change_count} consecutive epochs.\")\n","            if no_change_count >= max_no_change_epochs:\n","                print(f\"Stopping criterion met: No change in assignments for {max_no_change_epochs} consecutive epochs.\")\n","                break\n","        else:\n","            no_change_count = 0  \n","        cluster_assignments = new_cluster_assignments  \n","\n","        avg_loss = epoch_loss / len(train_loader.dataset)\n","        print(f\"(FullCALMModel) Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n","        torch.cuda.empty_cache()  \n","\n","    print(\"FullCALMModel Training complete.\")"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":201031,"status":"ok","timestamp":1731269857623,"user":{"displayName":"Caroline Gilbert","userId":"01576484869533418489"},"user_tz":300},"id":"EeI0Hcu5tNho","outputId":"df2f6a0f-88df-4a3f-ca21-577e09f8e889"},"outputs":[{"ename":"UnpicklingError","evalue":"invalid load key, '<'.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 146\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_test_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_test_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Load and combine the original train and test datasets for reshuffling and splitting\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Data/train_dataset.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m test_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Data/test_dataset.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    148\u001b[0m combined_dataset \u001b[38;5;241m=\u001b[39m ConcatDataset([train_data, test_data])\n","File \u001b[0;32m~/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/torch/serialization.py:1040\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/Purdue/Courses/2024-25/FALL24/ECE570/Project/ECE570-Music-Genre-Classification-CALM/calmenv/lib/python3.11/site-packages/torch/serialization.py:1258\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1258\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '<'."]}],"source":["def evaluate_model(model, dataloader, criterion):\n","    model.cnn_lstm.eval()\n","    model.eval()\n","    total_loss = 0.0\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for batch_idx, (inputs, labels) in enumerate(dataloader):\n","            inputs, labels = inputs.to(model.device), labels.to(model.device)\n","\n","            # Get predictions and calculate loss\n","            outputs, _ = model(inputs, labels)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item() * inputs.size(0)\n","\n","            # Predicted labels\n","            _, predicted = torch.max(outputs, dim=1)\n","            correct_predictions += (predicted == labels).sum().item()\n","            total_samples += labels.size(0)\n","\n","            # Debugging information for each batch\n","            print(f\"Batch {batch_idx + 1}: Loss = {loss.item():.4f}, Correct = {(predicted == labels).sum().item()}/{labels.size(0)}\")\n","\n","    # Calculate average loss and accuracy\n","    avg_loss = total_loss / total_samples\n","    accuracy = (correct_predictions / total_samples) * 100\n","\n","    # Output results for validation\n","    print(f\"Total Samples: {total_samples}, Total Correct: {correct_predictions}\")\n","    print(f\"Average Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n","\n","    return avg_loss, accuracy\n","\n","# Function to shuffle, split, and evaluate with an 8:1:1 ratio\n","def shuffle_and_resplit_evaluate(model_class, combined_dataset, batch_size=32, num_epochs=150):\n","    total_size = len(combined_dataset)\n","    train_size = int(0.8 * total_size)\n","    val_size = int(0.1 * total_size)\n","    test_size = total_size - train_size - val_size\n","\n","    # Shuffle and split\n","    train_dataset, val_dataset, test_dataset = random_split(combined_dataset, [train_size, val_size, test_size])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Split the training dataset into two halves\n","    train_size = len(train_loader.dataset) // 1\n","    remaining_size = len(train_loader.dataset) - train_size\n","    train_subset, _ = random_split(train_loader.dataset, [train_size, remaining_size])\n","\n","    # Define a DataLoader for just the train subset\n","    train_subset_loader = DataLoader(train_subset, batch_size=config['batch_size'], shuffle=True)\n","\n","    # Train CNN_LSTM on the subset\n","    cnn_lstm_model = CNN_LSTM().to(device)\n","    trainCNN_LSTM(cnn_lstm_model, train_subset_loader, device, num_epochs=1)  # Use limited epochs\n","\n","    # Freeze CNN_LSTM parameters after training on subset\n","    for param in cnn_lstm_model.parameters():\n","        param.requires_grad = False\n","\n","\n","    model = FullCALMModel(cnn_lstm_model, device).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n","    criterion = torch.nn.CrossEntropyLoss()\n","\n","    print(\"Training on reshuffled dataset...\")\n","    train_full_model(model, train_loader, optimizer, criterion, num_epochs=num_epochs)\n","\n","    # Evaluate\n","    val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n","    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n","\n","    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n","    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n","\n","# K-fold cross-validation with an 8:1 split within training data for validation\n","def k_fold_cross_validation(model_class, combined_dataset, k=5, batch_size=32, num_epochs=150):\n","    kf = KFold(n_splits=k, shuffle=True)\n","    fold_results = []\n","\n","    for fold, (train_val_indices, test_indices) in enumerate(kf.split(combined_dataset)):\n","        print(f\"\\nStarting Fold {fold + 1}/{k}\")\n","\n","        train_val_subset = Subset(combined_dataset, train_val_indices)\n","        test_subset = Subset(combined_dataset, test_indices)\n","\n","        # 8:1 split for train and validation\n","        train_size = int(0.9 * len(train_val_subset))\n","        val_size = len(train_val_subset) - train_size\n","\n","        train_subset, val_subset = random_split(train_val_subset, [train_size, val_size])\n","\n","        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n","        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n","        test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n","\n","        train_size = len(train_loader.dataset) // 1\n","        remaining_size = len(train_loader.dataset) - train_size\n","        train_subset, _ = random_split(train_loader.dataset, [train_size, remaining_size])\n","\n","        # Define a DataLoader for just the train subset\n","        train_subset_loader = DataLoader(train_subset, batch_size=config['batch_size'], shuffle=True)\n","\n","        # Train CNN_LSTM on the subset\n","        cnn_lstm_model = CNN_LSTM().to(device)\n","        trainCNN_LSTM(cnn_lstm_model, train_subset_loader, device, num_epochs=1)  # Use limited epochs\n","\n","        # Freeze CNN_LSTM parameters after training on subset\n","        for param in cnn_lstm_model.parameters():\n","            param.requires_grad = False\n","\n","\n","        model = model_class(cnn_lstm_model, device).to(device)\n","        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n","        criterion = torch.nn.CrossEntropyLoss()\n","\n","        train_full_model(model, train_loader, optimizer, criterion, num_epochs=num_epochs)\n","\n","        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n","        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n","\n","        print(f\"Fold {fold + 1} Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n","        print(f\"Fold {fold + 1} Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n","\n","        fold_results.append((val_loss, val_accuracy, test_loss, test_accuracy))\n","\n","        # Clear memory\n","        del model\n","        torch.cuda.empty_cache()\n","\n","    avg_val_loss = sum([result[0] for result in fold_results]) / k\n","    avg_val_accuracy = sum([result[1] for result in fold_results]) / k\n","    avg_test_loss = sum([result[2] for result in fold_results]) / k\n","    avg_test_accuracy = sum([result[3] for result in fold_results]) / k\n","\n","    print(f\"\\nK-Fold Cross-Validation Results ({k} folds):\")\n","    print(f\"Average Validation Loss: {avg_val_loss:.4f}, Average Validation Accuracy: {avg_val_accuracy:.2f}%\")\n","    print(f\"Average Test Loss: {avg_test_loss:.4f}, Average Test Accuracy: {avg_test_accuracy:.2f}%\")\n","\n","\n","# Load and combine the original train and test datasets for reshuffling and splitting\n","train_data = torch.load('./Data/Dataset/train_dataset.pt')\n","test_data = torch.load('./Data/Dataset/test_dataset.pt')\n","combined_dataset = ConcatDataset([train_data, test_data])\n","\n","# Usage example\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Perform reshuffle and resplit\n","shuffle_and_resplit_evaluate(FullCALMModel, combined_dataset, batch_size=32, num_epochs=150)\n","\n","# Perform k-fold cross-validation\n","k_fold_cross_validation(FullCALMModel, combined_dataset, k=5, batch_size=32, num_epochs=150)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8nmOD8e-aDMw"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM8HsqiRZQXG6xuuOVHzh1Y","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
